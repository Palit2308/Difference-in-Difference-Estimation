{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\sneha\\Downloads\\cps_00006.csv.gz' # path for the data, needs to be changed accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_states = 50     # this needs be changed to the desired number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "\n",
    "# data preperation code\n",
    "\n",
    "def cps_data(file_path):\n",
    "  \n",
    "    df = pd.read_csv(file_path, compression='gzip', header=0) # importing the data\n",
    "\n",
    "    \n",
    "    df = df[(df['INCWAGE'] != 99999999) & (df['INCWAGE'] != 0) & (df['INCWAGE'] != 999)]  # dropping the rows containing invalid and 0 INCWAGE according to the labels\n",
    "\n",
    "    df['INCWAGE'] = np.log(df['INCWAGE'])  # taking log of weekly earnings which will be our dependent variable in the regressions\n",
    "\n",
    "\n",
    "    df = df[(df['EDUC'] != 0) & (df['EDUC'] != 1)]  # dropping education levels 0 and 1 as they are invalid entries as per the labels of the dataset\n",
    "\n",
    "    df = df[(df['YEAR'] >= 1980) & (df['YEAR'] <= 2000)]  # taking the time frame from 1980 to 2000\n",
    "\n",
    "    def categorize_education(educ_code): # creating a fucntion to categorize education levels\n",
    "        if educ_code <= 10:\n",
    "            return 'Up to Grade 10'\n",
    "        elif 10 < educ_code <= 70:\n",
    "            return 'High School'\n",
    "        elif 70 < educ_code <= 123:\n",
    "            return \"Master's Degree\"\n",
    "        else:\n",
    "            return 'Doctorate Degree'\n",
    "\n",
    "    \n",
    "    df['Education_Category'] = df['EDUC'].apply(categorize_education) # applying the function to create a new 'Education_Category' column\n",
    "\n",
    "    df = pd.get_dummies(df, columns=['Education_Category'], prefix='', prefix_sep='', drop_first=True)\n",
    "\n",
    "    boolean = ['Up to Grade 10', 'High School', \"Master's Degree\"]\n",
    "    df[boolean] = df[boolean].astype(int)\n",
    "\n",
    "    df = df[~((df['STATEFIP'] > 56) | (df['STATEFIP'] == 11))]  # taking only the 50 states of the States and exclusing the regions as per the labels of the dataset\n",
    "\n",
    "    df = df[(df['AGE'] >= 25) & (df['AGE'] <= 50)]  # taking the age group from 25 to 50\n",
    "\n",
    "    df = df[df['SEX'] == 2] # taking only female respondents\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# data aggregation code\n",
    "\n",
    "def process_cps_data(file_path):\n",
    "\n",
    "    df = cps_data(file_path)\n",
    "\n",
    "    X = df[['High School', \"Master's Degree\", 'Up to Grade 10', 'AGE']] # the covariates used 1st stage of data aggregation\n",
    "    y = df['INCWAGE']\n",
    "\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    y_pred = model.predict(X)  # obtaining the predicted values of the model\n",
    "\n",
    "    residuals = y - y_pred\n",
    "\n",
    "    df['Residuals'] = residuals\n",
    "\n",
    "    residuals_mean_by_state_year = df.groupby(['STATEFIP', 'YEAR'])['Residuals'].mean().reset_index()\n",
    "\n",
    "    residuals_mean_by_state_year \n",
    "\n",
    "    return residuals_mean_by_state_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "alpha = 0.05  # Significance level\n",
    "num_simulations = 1000\n",
    "\n",
    "\n",
    "reject_count = 0  # Counter for the number of rejections\n",
    "true_beta1_value = 0   # Initialising the counters\n",
    "bias_values = []\n",
    "squared_error_values = []\n",
    "standard_error_values =[]\n",
    "beta1_estimates = []\n",
    "\n",
    "\n",
    "np.random.seed(42) # setting seed for reproducibility\n",
    "\n",
    "df = process_cps_data(file_path)  # importing the data from process_cps_data module\n",
    "\n",
    "for _ in range(num_simulations):\n",
    "\n",
    "    data = df.copy()\n",
    "\n",
    "    unique_statefips = data['STATEFIP'].unique()\n",
    "    selected_statefips = np.random.choice(unique_statefips, size=num_of_states, replace=False)\n",
    "\n",
    "    # Filter the DataFrame to only include rows with the selected STATEFIPs\n",
    "    data = data[data['STATEFIP'].isin(selected_statefips)]\n",
    "    \n",
    "    states = data['STATEFIP'].unique()\n",
    "    \n",
    "    # Randomly selecting  half of the states to be in the treatment group\n",
    "    \n",
    "    treatment_states = np.random.choice(states, size=len(states)//2, replace=False)\n",
    "\n",
    "    # Assigning treatment year to each treatment state, staggered between 1985 and 1995\n",
    "\n",
    "    treatment_years = np.random.choice(range(1985, 1995), size=len(treatment_states), replace=True)\n",
    "    state_to_treatment_year = dict(zip(treatment_states, treatment_years))\n",
    "\n",
    "    # Adding a treatment column to the DataFrame\n",
    "\n",
    "    data['TREATMENT'] = data.apply(lambda x: 1 if x['STATEFIP'] in treatment_states and x['YEAR'] >= state_to_treatment_year[x['STATEFIP']] else 0, axis=1)\n",
    "  \n",
    "    dummy_df = pd.get_dummies(data['STATEFIP'], prefix='STATEFIP', drop_first=True).astype(int)\n",
    "\n",
    "    # Concatenate the dummy variables with the original DataFrame\n",
    "    data = pd.concat([data, dummy_df], axis=1)\n",
    "\n",
    "    dummy_df2 = pd.get_dummies(data['YEAR'], prefix='YEAR', drop_first=True).astype(int)\n",
    "\n",
    "    # Concatenate the dummy variables with the original DataFrame\n",
    "    data = pd.concat([data, dummy_df2], axis=1)\n",
    "\n",
    "    columns_to_exclude = ['STATEFIP', 'YEAR', 'Residuals']\n",
    "\n",
    "    X_columns =[col for col in data.columns if col not in columns_to_exclude]\n",
    "\n",
    "    X = data[X_columns]\n",
    "\n",
    "    X = sm.add_constant(X)\n",
    "    Y = data['Residuals'] \n",
    "    model = sm.OLS(Y, X).fit(cov_type='cluster', use_t = True,  cov_kwds={'groups': data['STATEFIP'].astype(str)})\n",
    "\n",
    "    bias_value = model.params['TREATMENT'] - true_beta1_value\n",
    "    \n",
    "    bias_values.append(bias_value)\n",
    "\n",
    "    squared_error = (model.params['TREATMENT'] - true_beta1_value) ** 2\n",
    "\n",
    "    \n",
    "    standard_error = model.bse['TREATMENT']\n",
    "    standard_error_values.append(standard_error)\n",
    "    beta1_estimates.append(model.params['TREATMENT'])\n",
    "    \n",
    "    \n",
    "    # Checking if null hypothesis for beta1 is rejected\n",
    "\n",
    "    if model.pvalues['TREATMENT'] < alpha :\n",
    "        reject_count += 1\n",
    "\n",
    "\n",
    "type1_error = reject_count / num_simulations\n",
    "\n",
    "bias = np.mean(bias_values)\n",
    "mse = np.mean(squared_error)   \n",
    "rmse = np.sqrt(mse)  \n",
    "average_standard_error = np.mean(standard_error_values)   \n",
    "std_error_beta_distribution = np.std(beta1_estimates)\n",
    "\n",
    "confidence_interval = (\n",
    "    np.mean(beta1_estimates) - 1.96 * std_error_beta_distribution,\n",
    "    np.mean(beta1_estimates) + 1.96 * std_error_beta_distribution\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Number of times null hypothesis is rejected: {reject_count} out of {num_simulations} simulations\")\n",
    "print(f\"Type 1 Error: {type1_error}\")\n",
    "print(f\"Bias for Coefficient of Treatment : {bias}\")\n",
    "print(f\"MSE for Coefficient of Treatment : {mse}\")\n",
    "print(f\"RMSE for Coefficient of Treatment : {rmse}\")\n",
    "print(f\"Average Standard Error: {average_standard_error}\")\n",
    "print(f\"Standard Error of the beta_1 distribution: {std_error_beta_distribution}\")\n",
    "print(f'The confidence interval is {confidence_interval[0] , {confidence_interval[1]}}')\n",
    "\n",
    "sns.histplot(beta1_estimates, kde = True)\n",
    "plt.xlabel(r'$\\beta_1$ Estimates')  \n",
    "plt.ylabel('Density')\n",
    "plt.title(r'Distribution of $\\beta_1$ Estimates')\n",
    "\n",
    "\n",
    "mean_beta1_estimate = np.mean(beta1_estimates) # Calculating mean of beta1 estimates for marking on plot\n",
    "\n",
    "# Adding vertical lines for the confidence interval and the mean beta1 estimate\n",
    "\n",
    "plt.axvline(x=confidence_interval[0], color='red', linestyle='--', label='Lower CI')\n",
    "plt.axvline(x=confidence_interval[1], color='green', linestyle='--', label='Upper CI')\n",
    "plt.axvline(x=mean_beta1_estimate, color='blue', linestyle='-', label='Mean Beta1 Estimate')\n",
    "\n",
    "plt.legend(loc='lower left', bbox_to_anchor=(1, 0))\n",
    "\n",
    "plt.grid(True, which='both', linestyle='-', linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
